# -*- coding: utf-8 -*-
import numpy as np
import torch
import copy
import torch.nn.functional as F

from ncarrara.bftq_pydial.bftq.budgeted_utils import BudgetedUtils, TransitionBudgeted
from ncarrara.utils.torch import optimizer_factory
from ncarrara.utils_rl.transition.replay_memory import Memory
from ncarrara.utils_rl.transition.transition import TransitionGym
import logging

from ncarrara.utils_rl.visualization.toolsbox import fast_create_Q_histograms_for_actions

logger = logging.getLogger(__name__)


class PytorchBudgetedDQN():

    def __init__(self,

                 policy_net,
                 beta_for_discretisation,
                 workspace,
                 device,
                 loss_function,
                 loss_function_c,
                 optimizer_params={"learning_rate": 0.01,
                                   "weight_decay": 0.0,
                                   "optimizer_type": "ADAM"},
                 gamma=0.999,
                 gamma_c=1.0,
                 weights_losses=[1., 1.],
                 target_update=10,
                 size_minibatch=32,
                 state_to_unique_str="id",
                 action_to_unique_str="id",
                 actions_str=None
                 ):
        # TODO refaire proprement avec factory et functions
        if state_to_unique_str == "str":
            state_to_unique_str = lambda a: str(a)
        else:
            raise Exception("Unknown state_to_unique_str {}".format(state_to_unique_str))
        if action_to_unique_str == "str":
            action_to_unique_str = lambda a: str(a)
        else:
            raise Exception("Unknown action_to_unique_str {}".format(action_to_unique_str))

        # pattern, par composition (on ne veut pas d hÃ©ritage pour les algos)
        self.budgeted_utils = BudgetedUtils(beta_for_discretization=beta_for_discretisation,
                                            workspace=workspace,
                                            device=device,
                                            loss_function_c=loss_function_c,
                                            loss_function=loss_function,
                                            id="NO_ID")
        self.device = device
        self.optimizer_params = optimizer_params
        self.state_to_unique_str = state_to_unique_str
        self.action_to_unique_str = action_to_unique_str
        self.gamma = gamma
        self.gamma_c = gamma_c
        self.size_mini_batch = size_minibatch
        self.target_update = target_update
        self.weights_losses = weights_losses
        self.policy_net = policy_net
        self.memory = Memory(class_transition=TransitionBudgeted)
        self.i_episode = 0
        self.hulls_key_id = {}
        self.N_actions = policy_net.predict.out_features // 2
        if actions_str is None:
            self.actions_str = [str(a) for a in range(self.N_actions)]
        else:
            self.actions_str = actions_str
        logger.info('actions_str={}'.format(self.actions_str))
        logger.info('N_actions={}'.format(self.N_actions))
        self.reset()

    def reset(self):
        self.budgeted_utils.reset()
        self.memory.reset()
        self.target_net = copy.deepcopy(self.policy_net)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        print(self.optimizer_params)
        self.optimizer = optimizer_factory(params=self.policy_net.parameters(), **self.optimizer_params)
        self.i_episode = 0
        self.current_hull_id = 0
        self.i_update = 0
        self.hulls_key_id = {}

    def _optimize_model(self):
        transitions = self.memory.sample(
            len(self.memory) if self.size_mini_batch > len(self.memory) else self.size_mini_batch)
        batch = TransitionBudgeted(*zip(*transitions))
        loss = self.budgeted_utils.loss(Q=self.policy_net, batch=batch)
        self.optimizer.zero_grad()
        loss.backward()
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()

    def pi(self, state, beta, action_mask):
        return self.budgeted_utils.policy(self.policy_net, state, beta, action_mask)

    def update(self, *sample):
        self.budgeted_utils.change_id(self.i_update)
        state, action, reward, next_state, constraint, beta, done, info = sample
        state = torch.tensor(state, device=self.device, dtype=torch.float).unsqueeze(0)
        if not done:
            next_state = torch.tensor(next_state, device=self.device, dtype=torch.float).unsqueeze(0)
        else:
            next_state = None
        action = torch.tensor(action, device=self.device, dtype=torch.long).unsqueeze(0)
        reward = torch.tensor(float(reward), device=self.device, dtype=torch.float).unsqueeze(0)
        constraint = torch.tensor(float(constraint), device=self.device, dtype=torch.float).unsqueeze(0)
        beta = torch.tensor(float(beta), device=self.device, dtype=torch.float).unsqueeze(0)

        hull_key = self.state_to_unique_str(next_state)
        if hull_key in self.hulls_key_id:
            hull_id = self.hulls_key_id[hull_key]
        else:
            hull_id = self.current_hull_id
            self.current_hull_id += 1
        hull_id = torch.tensor(hull_id, device=self.device, dtype=torch.int).unsqueeze(0)
        self.memory.push(state, action, reward, next_state, constraint, beta, hull_id)
        self._optimize_model()
        if next_state is None:
            self.i_episode += 1
            if self.i_episode % self.target_update == 0:
                logger.info("[update][i_episode={}] copying weights to target network".format(self.i_episode))
                self.target_net.load_state_dict(self.policy_net.state_dict())
                if logger.getEffectiveLevel() is logging.INFO:
                    if self.i_episode % 10 == 0:
                        with torch.no_grad():
                            logger.info("Creating histograms ...")
                            zipped = TransitionBudgeted(*zip(*self.memory.memory))
                            state_batch = torch.cat(zipped.state)
                            beta_batch = torch.cat(zipped.beta_batch)
                            state_beta_batch = torch.cat((state_batch, beta_batch), dim=2)
                            QQ = self.policy_net(state_beta_batch)
                            QQr = QQ[:, 0:self.N_actions]
                            QQc = QQ[:, self.N_actions:2*self.N_actions]
                            mask_action = np.zeros(self.N_actions)
                            fast_create_Q_histograms_for_actions(
                                title="actions_Qr(s)_pred_target_e={}".format(self.i_episode),
                                QQ=QQr.cpu().numpy(),
                                path=self.workspace + "/histogram",
                                labels=self.actions_str,
                                mask_action=mask_action)
                            fast_create_Q_histograms_for_actions(
                                title="actions_Qc(s)_pred_target_e={}".format(self.i_episode),
                                QQ=QQc.cpu().numpy(),
                                path=self.workspace + "/histogram",
                                labels=self.actions_str,
                                mask_action=mask_action)

        self.i_update += 1
