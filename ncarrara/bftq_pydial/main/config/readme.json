{
  "general": {
    "id": "camera_ready_7",
    "seed": 0,
    "workspace": "tmp/camera_ready_7",
    "dictConfig": {
      "version": 1,
      "disable_existing_loggers": false,
      "formatters": {
        "standard": {
          "format": "[%(levelname)s] %(message)s"
        }
      },
      "handlers": {
        "default": {
          "level": "INFO",
          "formatter": "standard",
          "class": "logging.StreamHandler"
        }
      },
      "loggers": {
        "": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.bftq_pydial.tools.features": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.utils_rl.algorithms.dqn": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.bftq_pydial.bftq.pytorch_budgeted_fittedq": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.utils_rl.algorithms.pytorch_fittedq": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.utils_rl.environments.slot_filling_env.slot_filling_env": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.utils.math": {
          "handlers": [
            "default"
          ],
          "level": "WARN",
          "propagate": false
        },
        "ncarrara.utils.os": {
          "handlers": [
            "default"
          ],
          "level": "ERROR",
          "propagate": false
        }
      }
    }
  },
  "feature_str": "feature_basic",
  "generate_envs": {
    "envs_str": "slot_filling_env_v0",
    "envs_params": {
      "params1": [0,1,2] // if list, will generate combinaison of envs
    }
  },
  "create_data": { // parameters for run_dqn or create_data
    "filename_data": "samples.data",
    "handcrafted_greedy_policy": true, // will use HDC policy instead of FTQ to create data with create_data.py
    "trajs_by_ftq_batch": 50, // number of trajectory generate between each batch of ftq (no used by dqn)
    "lambda_": 0, // lambda for reward penalty
    "normalize_reward": true,
    "N_trajs": 5000, // nb trajectory to generate
    "epsilon_decay": {
      "start": 1.0,
      "decay": 0.001
    }
  },
  "main": {
    "filename_data": "samples.data",
    "path_data": null, // if null, then fetch data in workspace, else fetch data to the path given
    "normalize_reward": true,
    "N_trajs": 1000 // number of test trajectory for FTQ/BFTQ policies
  },
  "bftq_params": {

    "nn_loss_stop_condition": 0.001, // stop condition during optimisation of network
    "max_nn_epoch": 2000,
    "optimizer": "ADAM",
    "loss_function": "l2",
    "max_ftq_epoch": 12,
    "learning_rate": 0.001,
    "reset_policy_each_ftq_epoch": true,  // should be true to avoid local minima
    "delta_stop": 0.001,
    "weight_decay": 0.0005,  // alpha for l2 regularization
    "batch_size_experience_replay": "ALL_BATCH",
    "loss_function_c": "l2",
    "weights_losses": [ // apply a weight for constraint loss and reward loss (use it if one or the other is not well learned"
      1.0,
      1.0
    ]
  },
  "bftq_net_params": {
    "beta_encoder_type": "LINEAR", // LINEAR for a real vector encoding, REPEAT to duplicate the beta value in a vector
    "size_beta_encoder": 50,
    "activation_type": "RELU",
    "reset_type": "XAVIER",
    "intra_layers": [
      256,
      128,
      64
    ]
  },
  "nb_trajs_between_epoch": 30, // number of test trajectory to plot FTQ performances between epoch. For logging purpose only
  "lambdas": "[0.,10.,15.,20.,25.,30,40.,50.,75.,100.,250.,500.]",
  "betas": "np.linspace(0.,1.,26)", // beta used to duplication samples (for bftq learning)
  "betas_test": "np.linspace(0.,1.,26)",
  "ftq_params": {
    "nn_loss_stop_condition": 0.001,
    "max_nn_epoch": 2000,
    "optimizer": "ADAM",
    "loss_function": "l2",
    "max_ftq_epoch": 12,
    "learning_rate": 0.001,
    "reset_policy_each_ftq_epoch": true, // should be true to avoid local minima
    "delta_stop": 0.001,
    "weight_decay": 0.0005,
    "batch_size_experience_replay": "ALL_BATCH" // at each epoch, the batch is sampled. "ALL_BATCH" uses the whole batch to learn, "ADAPTATIVE" use 10% of the batch, and "xxx" use xxx samples.
  },
  "net_params": {
    "activation_type": "RELU",
    "reset_type": "XAVIER",
    "intra_layers": [
      128,
      64,
      32
    ]
  },
  "gamma": 1.0,
  "gamma_c": 1.0,
  "dqn_params": {
    "gamma": 1, // bug, must be same gamma as the line up
    "batch_size_experience_replay": 32,
    "target_update": 10, // number of trajectory between each update of target network
    "optimizer": "ADAM",
    "loss_function": "l2",
    "lr": 0.001, // learning rate
    "weight_decay": 0.0005,
    "workspace": "dqn" // deprecated, should be overrride
  }
}


